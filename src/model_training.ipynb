{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27f516",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import utils\n",
    "from sklearn.impute import SimpleImputer\n",
    "from visualization_and_analysis import create_matplotlib_table, plot_confusion_matrix, plot_feature_importances\n",
    "import os\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d68143",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Get X (features) and y (target)\n",
    "def get_X_y(data):\n",
    "    X = data.drop(['veracity', 'nationality', 'position', 'source'], axis=1)\n",
    "    y = data['veracity']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43310390",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Impute missing values using mean imputation\n",
    "def impute_missing_values(X):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    X_imputed = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "    return X_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443007a0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Apply Random Over Sampling\n",
    "def apply_ros(X_train, y_train):\n",
    "    print(\"\\nApplying Random Over Sampling...\")\n",
    "    print(f\"Before applying ROS, the number of samples in the minority class: {y_train.value_counts()[0]}\")\n",
    "    print(f\"Before applying ROS, the number of samples in the majority class: {y_train.value_counts()[1]}\")\n",
    "\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(f\"After applying ROS, the number of samples in the minority class: {y_train_resampled.value_counts()[0]}\")\n",
    "    print(f\"After applying ROS, the number of samples in the majority class: {y_train_resampled.value_counts()[1]}\")\n",
    "\n",
    "    return X_train_resampled, y_train_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076baf63",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Split data into train and test sets. Oversamples the minority class if needed\n",
    "def split_data(data, oversample=False):\n",
    "    X, y = get_X_y(data)\n",
    "    X_imputed = impute_missing_values(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.4, random_state=42)\n",
    "\n",
    "    if oversample:\n",
    "        X_train_resampled, y_train_resampled = apply_ros(X_train, y_train)\n",
    "        return X_train_resampled, X_test, y_train_resampled, y_test\n",
    "    else:\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9285743",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train and evaluate the model on the test set\n",
    "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, cv):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Perform k-fold cross-validation and compute mean accuracy\n",
    "    cv_accuracy = np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy'))\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    # Add cross-validated accuracy to the report dictionary\n",
    "    report['cross_validated_accuracy'] = cv_accuracy\n",
    "\n",
    "    return accuracy, cv_accuracy, report, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dfcbf7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Top 5 feature importances for the model from the training set\n",
    "def top_5_feature_importances(model, X_train):\n",
    "    total_features = len(X_train.columns)\n",
    "    feature_importances = sorted(zip(X_train.columns, model.feature_importances_), key=lambda x: x[1], reverse=True)\n",
    "    top_5_features = feature_importances[:5]\n",
    "\n",
    "    return total_features, top_5_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0b746",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def convert_top_5_feature_importances_to_df(top_5_features):\n",
    "    return pd.DataFrame(top_5_features, columns=['Feature', 'Importance']).set_index('Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e3750",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_top_5_feature_importances(model_name, model, X_train):\n",
    "    total_features, top_5_features = top_5_feature_importances(model, X_train)\n",
    "    print(f\"\\n{model_name} top 5 features out of {total_features} features:\")\n",
    "    for feature, importance in top_5_features:\n",
    "        print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd30d5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def convert_model_report_to_df(report):\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df = report_df.drop(columns=['support'])\n",
    "    report_df = report_df.drop(['macro avg', 'weighted avg'])\n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44765eb1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_model_results(model_name, accuracy, cv_accuracy, report_df):\n",
    "    print(f\"\\n{model_name} cross-validated accuracy: {cv_accuracy:.4f}\")\n",
    "    print(f\"{model_name} accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification report table:\")\n",
    "    print(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938fed19",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(model_name, y_test, y_pred):\n",
    "    print(f\"{model_name} confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569d371",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def show_model_report_results(model_name, accuracy, cv_accuracy, report, save_path=None):\n",
    "    report_df = convert_model_report_to_df(report).round(4)\n",
    "    print_model_results(model_name, accuracy, cv_accuracy, report_df)\n",
    "    if not save_path:\n",
    "        save_path = os.path.join('results', f'{model_name.lower()}_classification_report.png')\n",
    "\n",
    "    create_matplotlib_table(report_df, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0b8b1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Show the feature importances in a bar chart, a table and save them\n",
    "def show_feature_importance_results(model_name, model, X_train):\n",
    "    print_top_5_feature_importances(model_name, model, X_train)\n",
    "    top_5_features = top_5_feature_importances(model, X_train)[1]\n",
    "    top_5_importances_df = convert_top_5_feature_importances_to_df(top_5_features).round(4)\n",
    "    if_save_path = os.path.join('results', f'{model_name.lower()}_top_5_features.png')\n",
    "    create_matplotlib_table(top_5_importances_df, if_save_path)\n",
    "    if_bar_chart_save_path = os.path.join('results', f'{model_name.lower()}_top_5_features_bar_chart.png')\n",
    "    plot_feature_importances(model_name, model, X_train, if_bar_chart_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ed028",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Show the confusion matrix in a table and a plot and save the plot\n",
    "def show_confusion_matrix_results(model_name, y_test, y_pred, save_path=None):\n",
    "    print_confusion_matrix(model_name, y_test, y_pred)\n",
    "    if not save_path:\n",
    "        save_path = os.path.join('results', f'{model_name.lower()}_confusion_matrix.png')\n",
    "    plot_confusion_matrix(model_name, y_test, y_pred, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1701f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Trains, evaluates, and compares models using given data, cross-validation, and data type, returning the best model and its accuracy.\n",
    "def evaluate_models_on_data(models, X_train, X_test, y_train, y_test, cv, data_type='Original'):\n",
    "    best_model = None\n",
    "    best_model_name = None\n",
    "    best_accuracy = 0\n",
    "    report_save_path = None\n",
    "    confusion_save_path = None\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n{model_name} model ({data_type} data):\")\n",
    "        accuracy, cv_accuracy, report, y_pred = train_and_evaluate_model(model, X_train, X_test, y_train, y_test, cv)\n",
    "        if data_type != \"Original\":\n",
    "            report_save_path = os.path.join('results', f'{model_name.lower()}_{data_type.lower()}_classification_report.png')\n",
    "            confusion_save_path = os.path.join('results', f'{model_name.lower()}_{data_type.lower()}_confusion_matrix.png')\n",
    "        show_model_report_results(model_name, accuracy, cv_accuracy, report, save_path=report_save_path)\n",
    "        show_confusion_matrix_results(model_name, y_test, y_pred, save_path=confusion_save_path)\n",
    "\n",
    "        if accuracy > best_accuracy and data_type == 'Original':\n",
    "            best_model = model\n",
    "            best_model_name = model_name\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "    return best_model, best_model_name, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits input data, oversamples if needed, and evaluates various classifiers on the data, reporting performance and feature importances.\n",
    "def train_and_evaluate_models(data):\n",
    "    X_train, X_test, y_train, y_test = split_data(data)\n",
    "    X_train_resampled, _, y_train_resampled, _ = split_data(data, oversample=True)\n",
    "\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "    }\n",
    "\n",
    "    # Using StratifiedKFold to maintain class distribution across folds\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    best_model, best_model_name, _ = evaluate_models_on_data(models, X_train, X_test, y_train, y_test, cv, data_type='Original')\n",
    "    evaluate_models_on_data(models, X_train_resampled, X_test, y_train_resampled, y_test, cv, data_type='Oversampled')\n",
    "\n",
    "    print(f\"\\n===================== Best model: {best_model_name} =====================\")\n",
    "\n",
    "    show_feature_importance_results(best_model_name, best_model, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa56a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    output_data = utils.pandas_load_csv(\"output_data.csv\")\n",
    "    \n",
    "    train_and_evaluate_models(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25788367",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
